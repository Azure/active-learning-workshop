{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurizing Toxic Comments Using Pre-Trained Word Vectors and a Language Model's Encoder\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides an analysis of featurization methods for text. The central idea we examine is that we can represent text (words, phrases, and even entire sentences or paragraphs) as vectors. However, we'll see that some vector representations may provide more semantic information than others. \n",
    "\n",
    "## Pre-Trained Word Vectors\n",
    "\n",
    "As a first foray we featurize our data using publically available pre-trained word vectors. There are numerous word vectors available, the most common being [Word2Vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/), and [fasttext](https://github.com/facebookresearch/fastText/). We'll use the **GloVe** vectors trained on Wikipedia and Gigaword 5. \n",
    "\n",
    "Our input dataset contains a variable sequence of tokens (words), which we vectorize into a list of real-valued vectors. In order to use a machine learning model with such a representation we need to transform it into a fixed-vector representation. We can do this by many different aggregation schemes: sum/mean, max, min, etc. For this notebook we simply utilize unweighted averages of all the tokens, but you'll likely find that for some applications it may be more useful to consider max/min in addition, and concatenate multiple representations.\n",
    "\n",
    "\n",
    "![](https://image.slidesharecdn.com/starsem-170916142844/95/yejin-choi-2017-from-naive-physics-to-connotation-modeling-commonsense-in-frame-semantics-83-638.jpg?cb=1505572199)\n",
    "\n",
    "_image credit: Yejin Choi - 2017 - From Naive Physics to Connotation: Modeling Commonsense in Frame Semantics_\n",
    "\n",
    "_quote credit: Ray Mooney_\n",
    "\n",
    "\n",
    "## Language Model Encoders\n",
    "\n",
    "We'll then examine a more advanced method of featurizing our sequence of tokens. In particular, we'll use the encoder from a pre-trained language model. The encoder is a fixed-length vector representation that is typically the last hidden vector in a recurrent neural network trained for machine translation or language modeling.\n",
    "\n",
    "\n",
    "![](http://ruder.io/content/images/2018/07/lm_objective.png)\n",
    "\n",
    "_image credit: Seabstain Ruder and TheGradient: NLP's ImageNet moment has arrived_\n",
    "\n",
    "Our hope is that rather than naively aggregating our word vectors by their average representation, the last hidden layer will contain contextual information from the entire sequence of tokens.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:\n",
    "\n",
    "We import our dataset of comments to Wikipedia page-edits from our helper `load_data`. We'll also import a dictionary of GloVe vectors and a helper function for using it to lookup word vectors for our tokenized comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_wiki_attacks, load_attack_encoded\n",
    "from load_data import tokenize, create_glove_lookup, download_glove\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "data_dir = pathlib.Path.home() / \"active-learning-workshop\" / \"text_featurization\" / \"data\"\n",
    "if not data_dir.parent.exists():\n",
    "    data_dir.parent.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [spaCy](https://spacy.io/) library to tokenize our text, but aside from some prior data cleanup there's nothing fancy happening in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is not creative . Those are the diction...</td>\n",
       "      <td>[  , This, is, not, creative, ., Those, are, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the term standard model is itself less NPO...</td>\n",
       "      <td>[    , the, term, standard, model, is, itself,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>[  , True, or, false, ,, the, situation, as, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>[Next, ,, maybe, you, could, work, on, being, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[This, page, will, need, disambiguation, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Important note for all sysops There is a b...</td>\n",
       "      <td>[    , Important, note, for, all, sysops, Ther...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0    This is not creative . Those are the diction...   \n",
       "1      the term standard model is itself less NPO...   \n",
       "2    True or false, the situation as of March 200...   \n",
       "3  Next, maybe you could work on being less conde...   \n",
       "4                This page will need disambiguation.   \n",
       "5      Important note for all sysops There is a b...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [  , This, is, not, creative, ., Those, are, t...  \n",
       "1  [    , the, term, standard, model, is, itself,...  \n",
       "2  [  , True, or, false, ,, the, situation, as, o...  \n",
       "3  [Next, ,, maybe, you, could, work, on, being, ...  \n",
       "4        [This, page, will, need, disambiguation, .]  \n",
       "5  [    , Important, note, for, all, sysops, Ther...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_src =  str(data_dir / \"glove.6B.300d.txt\")\n",
    "\n",
    "if not pathlib.Path(glove_src).exists():\n",
    "    download_glove()\n",
    "glove_lookup = create_glove_lookup(glove_src)\n",
    "toxic_df = load_wiki_attacks()\n",
    "toxic_df = tokenize(toxic_df, \"comment_text\")\n",
    "toxic_df.loc[:5, ['comment_text', \"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize with GloVe:\n",
    "\n",
    "We can use our `glove_lookup` dictionary to vectorize all the tokens in our text. We apply the function to every token in our comment, and then take the average over all word vectors. Again, you should definitely consider other aggregation methods such as max/min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_df['glove_aggregate'] = toxic_df.tokens.apply(lambda x: np.mean([glove_lookup[v] for v in x], axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>glove_aggregate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is not creative . Those are the diction...</td>\n",
       "      <td>[  , This, is, not, creative, ., Those, are, t...</td>\n",
       "      <td>[-0.1499181361726886, 0.05423158062295079, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the term standard model is itself less NPO...</td>\n",
       "      <td>[    , the, term, standard, model, is, itself,...</td>\n",
       "      <td>[-0.1587900504010778, 0.013006055911443676, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>[  , True, or, false, ,, the, situation, as, o...</td>\n",
       "      <td>[-0.12243446450199647, -0.06342039171645089, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>[Next, ,, maybe, you, could, work, on, being, ...</td>\n",
       "      <td>[-0.12554458174387909, 0.0733292975111634, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[This, page, will, need, disambiguation, .]</td>\n",
       "      <td>[-0.3239130127453462, -0.058272738777694545, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Important note for all sysops There is a b...</td>\n",
       "      <td>[    , Important, note, for, all, sysops, Ther...</td>\n",
       "      <td>[-0.12507687575733267, 0.06996425342205934, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0    This is not creative . Those are the diction...   \n",
       "1      the term standard model is itself less NPO...   \n",
       "2    True or false, the situation as of March 200...   \n",
       "3  Next, maybe you could work on being less conde...   \n",
       "4                This page will need disambiguation.   \n",
       "5      Important note for all sysops There is a b...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [  , This, is, not, creative, ., Those, are, t...   \n",
       "1  [    , the, term, standard, model, is, itself,...   \n",
       "2  [  , True, or, false, ,, the, situation, as, o...   \n",
       "3  [Next, ,, maybe, you, could, work, on, being, ...   \n",
       "4        [This, page, will, need, disambiguation, .]   \n",
       "5  [    , Important, note, for, all, sysops, Ther...   \n",
       "\n",
       "                                     glove_aggregate  \n",
       "0  [-0.1499181361726886, 0.05423158062295079, -0....  \n",
       "1  [-0.1587900504010778, 0.013006055911443676, -0...  \n",
       "2  [-0.12243446450199647, -0.06342039171645089, 0...  \n",
       "3  [-0.12554458174387909, 0.0733292975111634, -0....  \n",
       "4  [-0.3239130127453462, -0.058272738777694545, -...  \n",
       "5  [-0.12507687575733267, 0.06996425342205934, -0...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.loc[:5, [\"comment_text\", \"tokens\", \"glove_aggregate\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model \n",
    "\n",
    "Our language model encoder utilizes pre-trained language models hosted on [TensorFlow Hub](https://www.tensorflow.org/hub/modules/text). \n",
    "\n",
    "Our helper script `encoder.py` provides a simple class entitled `encoder` with methods for encoding text using three different encoder models: [ELMO](http://www.aclweb.org/anthology/N18-1202), [USE](https://arxiv.org/pdf/1803.11175.pdf), and [NNLM](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Imports:\n",
    "\n",
    "The class is a bit verbose for readability, but it's conceptually very simple. We load the pre-trained module, which defines a static computational graph with the learned weights from the language model on it's dataset. We initialize this computational graph into a Keras session, which we can then use for fine-tuning or for featurizing an input sequence by computing a forward pass of the computational graph. Note, we could have also just used `tensorflow` directly to do the model building and training, but Keras has some helpful utilities for data min-batching and pre-fetching that makes this very easy (at the cost of some incompatibilities: [issues with fine-tuning may arise](https://groups.google.com/a/tensorflow.org/forum/#!topic/hub/Y4AdAM7HpX0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from encoder import encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'elmo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"elmo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Defines an encoder using a pre-trained TensorflowHub module. \u001b[0m\n",
       "\u001b[0;34m        Can be used for featurization or fine-tuned for classification.\u001b[0m\n",
       "\u001b[0;34m        \u001b[0m\n",
       "\u001b[0;34m        Parameters\u001b[0m\n",
       "\u001b[0;34m        ----------\u001b[0m\n",
       "\u001b[0;34m        model : str, optional\u001b[0m\n",
       "\u001b[0;34m            Trained model from tfhub to use (the default is \"elmo\")\u001b[0m\n",
       "\u001b[0;34m        trainable : bool, optional\u001b[0m\n",
       "\u001b[0;34m            Whether to fix weights or make them trainable (the default is False)\u001b[0m\n",
       "\u001b[0;34m        num_labels : int, optional\u001b[0m\n",
       "\u001b[0;34m            Number of labels in target class, applicable for classification (the default is 2)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Raises\u001b[0m\n",
       "\u001b[0;34m        ------\u001b[0m\n",
       "\u001b[0;34m        NotImplementedError\u001b[0m\n",
       "\u001b[0;34m            If self.model not in [\"elmo\", \"nnlm\", \"use\"]\u001b[0m\n",
       "\u001b[0;34m        \u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Create a TensorFlow Session and run initializers\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Embed string to lower dimensional vector\u001b[0m\n",
       "\u001b[0;34m        \u001b[0m\n",
       "\u001b[0;34m        Parameters\u001b[0m\n",
       "\u001b[0;34m        ----------\u001b[0m\n",
       "\u001b[0;34m        x : list\u001b[0m\n",
       "\u001b[0;34m            list of strings to tokenize and featurize\u001b[0m\n",
       "\u001b[0;34m        \u001b[0m\n",
       "\u001b[0;34m        Returns\u001b[0m\n",
       "\u001b[0;34m        -------\u001b[0m\n",
       "\u001b[0;34m        Trainable featurizer\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mELMO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/elmo/2\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mNNLM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/nnlm-en-dim128/1\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mUSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/universal-sentence-encoder-large/2\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"elmo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0melmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mELMO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mas_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nnlm\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mnnlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNNLM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"use\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                 \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mexecutable\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtransform_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Used for encoding a list of text sentence to a fixed vector.\u001b[0m\n",
       "\u001b[0;34m        \u001b[0m\n",
       "\u001b[0;34m        Returns\u001b[0m\n",
       "\u001b[0;34m        -------\u001b[0m\n",
       "\u001b[0;34m        Keras model for forward pass only\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"use\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0membed_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"elmo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0membed_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nnlm\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0membed_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                  \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                      \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtrainable_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"A trainable keras module for classification\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"use\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0membed_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"elmo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0membed_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nnlm\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0membed_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                  \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                      \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/active-learning-workshop/text_featurization/lm_finetune/encoder.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurized Dataset\n",
    "\n",
    "Here's an example usage of converting the `comment_text` into a fixed sequence using our encoder and the **Universal Sentence Encoder**:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "use_encoder = encoder(model=\"use\")\n",
    "featurizer = use_encoder.transform_model()\n",
    "featurizer.summary()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    transformed_review = featurizer.predict(toxic_df.comment_text.values, batch_size=64)\n",
    "```\n",
    "\n",
    "This operation will take some time, ~1.5 hours on a machine with 16 cores. We have a pre-featurized version of this dataset already saved for you, which you can download using our helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_attacks = load_attack_encoded()\n",
    "toxic_df['encoded_comment'] = encoded_attacks.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>encoded_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is not creative . Those are the diction...</td>\n",
       "      <td>[0.026781896, -0.05754256, 0.033774074, 0.0097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the term standard model is itself less NPO...</td>\n",
       "      <td>[0.011424046, -0.009576778000000001, -0.026437...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>[0.00041413374, 0.08557465, 0.024096673, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>[0.058229163, 0.048170675, -0.054312646, 0.029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[0.05511004, 0.017830834, -0.08593257, -0.0343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Important note for all sysops There is a b...</td>\n",
       "      <td>[0.07102389, 0.029018747, -0.04301559, -0.0764...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0    This is not creative . Those are the diction...   \n",
       "1      the term standard model is itself less NPO...   \n",
       "2    True or false, the situation as of March 200...   \n",
       "3  Next, maybe you could work on being less conde...   \n",
       "4                This page will need disambiguation.   \n",
       "5      Important note for all sysops There is a b...   \n",
       "\n",
       "                                     encoded_comment  \n",
       "0  [0.026781896, -0.05754256, 0.033774074, 0.0097...  \n",
       "1  [0.011424046, -0.009576778000000001, -0.026437...  \n",
       "2  [0.00041413374, 0.08557465, 0.024096673, -0.09...  \n",
       "3  [0.058229163, 0.048170675, -0.054312646, 0.029...  \n",
       "4  [0.05511004, 0.017830834, -0.08593257, -0.0343...  \n",
       "5  [0.07102389, 0.029018747, -0.04301559, -0.0764...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.loc[:5, ['comment_text', 'encoded_comment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "How do these features compare on discrimaniting between toxic / non-toxic comments? Let's put them in a raceoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "train_sizes = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "# estimator = GaussianNB()\n",
    "# estimator = RandomForestClassifier()\n",
    "\n",
    "def featurize(df=toxic_df):\n",
    "\n",
    "    labels = np.concatenate(lb.fit_transform(df.is_attack.values))\n",
    "    glove_features = np.vstack(df.glove_aggregate.values)\n",
    "    use_features = np.vstack(df.encoded_comment.values)\n",
    "\n",
    "    return labels, glove_features, use_features\n",
    "\n",
    "labels, glove_features, use_features = featurize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves:\n",
    "\n",
    "Learning curves allows us to visualize the performance of the system as a function of the amount of examples it's seen. We first create learning curves using the `glove_features`, and then we create learning curves of the `encoded_features`. We plot them together so we can compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 706 ms, total: 15 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g_train_sizes, g_train_scores, g_test_scores = learning_curve(estimator=estimator, \n",
    "                                                              X=glove_features,\n",
    "                                                              y=labels, \n",
    "                                                              scoring=make_scorer(roc_auc_score),\n",
    "                                                              n_jobs=8, train_sizes=train_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 s, sys: 1.24 s, total: 25.1 s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e_train_sizes, e_train_scores, e_test_scores = learning_curve(estimator=estimator, \n",
    "                                                              X=use_features,\n",
    "                                                              y=labels, \n",
    "                                                              scoring=make_scorer(roc_auc_score),\n",
    "                                                              n_jobs=8, train_sizes=train_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We used AUC as our scoring criteria, but you could also use accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3920f530f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAELCAYAAAAoUKpTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl4VfW59vHvk52EMAcQKHOiBQWBMGxBSxGtolitsxWqR9FWj6+i1dOeHm1t69F6jh1OW9uX0xarUq2KFiti1VrHQhWFoFgFxCIgBHkBmcoUkuz9vH+sRdgJSXaArOwM9+e69pU1/NbaT5a47qzfmszdERERqUtWpgsQEZGmT2EhIiJpKSxERCQthYWIiKSlsBARkbQUFiIikpbCQkRE0lJYiIhIWgoLERFJKzvTBTSUo446ygsKCjJdhohIs7J48eJP3b17unYtJiwKCgooLi7OdBkiIs2KmX1cn3bqhhIRkbQUFiIikpbCQkRE0lJYiIhIWgoLERFJS2EhIiJpKSxERCStFnOfhYi0IMkEJCsgUQ7J8mA8UR5MS5ZDMglmYFlpPmGbrFjdbSL7PZIpNVcc+L2qfKpPS/3d67lM+6NgyHnR/R5EHBZmNgm4F4gBv3X3e6rN7w/8DsgP29zq7s+Z2UTgHiAXKAP+3d1fibJWkVbJHcr3QNkeKNsFZbuDT/nuA8Nlu8L54XCVHVlF+vHDmYc37naoNUhidYcS1L0Tb6zfo0+8+YaFmcWA6cBEoARYZGZz3X1ZSrPbgSfc/VdmNgR4DigAPgW+5O6fmNlQ4AWgT1S1ijR57lCxr5Yd+e5D2NmH4+Up44eyQ8tuC7FciGVDVjZk5VQdzso+eDynbe3zDmU9sZThrFiwTTxZx2f//ESa+XUsn6xt2XA+HtaT+onVPh7LqWF+mmWyalqm2nh2blT/8g78p49w3WOAle6+CsDMZgHnAalh4UCncLgz8AmAu7+T0mYp0NbM2rj7vgjrFWlc7lC6HXZtCj67N8GuzbBr44Hh3fvnbYZEWf3XnZ0HOe0gtwPktg8/7aBd1wPjOe1T5lX/dAiXb39gHTntIEunOVurKMOiD7AuZbwEGFutzR3AX8zsRqA9cHoN67kIeFtBIc1CQwRAVja07x58OvSAHkOC4bxOVXf+OdV27rntDkyP6XSkNKxM/4uaAsx09/8xs5OAh81sqLsnAczseOCHwBk1LWxm1wLXAvTv37+RSpZWpyECwGLBjn9/AHQfHPzs0APa96g63LaL/oKXJifKsFgP9EsZ7xtOS/VVYBKAuy8wszzgKGCTmfUFngKucPePavoCd58BzACIx+ONfEZMWpREBexYC1tWwZaVsPUj2PJRMLxzQ+0BsH/nXxkA3aFDzzAAUoYVANLMRRkWi4CBZlZIEBKTga9Ua7MWOA2YaWaDgTxgs5nlA88SXB31eoQ1SmuSTMLOT4IA2PIRbF11YHjbmuDyxv1yO0K3o6HPaMjvF+78ewYBsH9YASCtSGRh4e4VZjaN4EqmGPCAuy81szuBYnefC3wDuM/MbiE42T3V3T1c7rPA98zse+Eqz3D3TVHVKy2Ee9AVtD8EKo8SVgXhULH3QNvsPOh6DPQ4Do47G7p9FrodE0zr0CPa6+9Fmhlzbxm9N/F43PXyo1Zkz9aqRwZbw2DYsgrKdh5ol5UDXQqCEOj2Weh69IHhjr11ZCCtnpktdvd4unaZPsEtUrt9O1MCodq5hL1bD7SzLOjcLwiAfmPDUDgm6Ebq3F9XBok0AP1fJE1D+V7Y8HdYvxjWF0NJMWyv9rbHTn2CI4Mh5x3oLur2WegyALLbZKZukVZCYSGNL5kMjhBKig8Ew8b3w8cjAJ36Qt/RMOoKOGpgEApdjw7uIxCRjFBYSPR2f1o1GD55G0p3BPNyO0KfkfC5m4Irj/rGoeNnMluviBxEYSENq7I7KQyG9YsPdCdZLLgb+fgLggef9Y3DUYOC59yISJOmsJDDl0wGJ53Xh6FQW3fSCV8LgqFXUfA4ChFpdhQWUn/17U7qGw+6lNSdJNJiKCykZgd1JxXD9rXBPItBzyFw/IUHzjOoO0mkRVNYyAHb18K7s+CDZ6t2J3XuF4TCCdeoO0mklVJYtHb7dsHyZ2DJI7BmfjCt/0nqThKRKhQWrVEyCR+/DksehWVPB29V61IIp94ORZdCvh73LiJVKSxak62rgm6mdx8LupzadIJhF8GIy4LHZOjBeSJSC4VFS1f6T1g2B5Y8BmvfAAyOORW+8L3gSau6K1pE6kFh0RIlE7B6XnAEsWxu8FjubgPhtO/D8Euhc59MVygizYzCoiX5dCW8+yi8+zj8swTyOsOIKUE3U5/R6mYSkcOmsGju9m6HpX8MuplKFgaP6/7s6XDGXXDsFyEnL9MVikgLoLBojpIJ+OjV4HLXD56FxL7g/c8T7wy6mXSpq4g0sEjDwswmAfcSvFb1t+5+T7X5/YHfAflhm1vd/blw3m3AV4EEcJO7vxBlrc3Cpg+Cbqa/PwE7NwTvgB59JRRNgd4j1c0kIpGJLCzMLAZMByYCJcAiM5vr7stSmt0OPOHuvzKzIcBzQEE4PBk4HugNvGRmg9w9EVW9TdaerfD+k8E9EZ+8HTxqY+AZcNaPYNCZeumPiDSKKI8sxgAr3X0VgJnNAs4DUsPCgU7hcGfgk3D4PGCWu+8DVpvZynB9CyKst+lIlMPKl4OjiBXPQ6IMeg6FM/8Lhl0CHXpkukIRaWWiDIs+wLqU8RJgbLU2dwB/MbMbgfbA6SnLvllt2ZZ/vefGpcERxN+fgN2boF234PHeRVOg1/BMVycirVimT3BPAWa6+/+Y2UnAw2Y2tL4Lm9m1wLUA/fs380dULJgOL3wbsnKC7qURl8HAiRDLyXRlIiKRhsV6oF/KeN9wWqqvApMA3H2BmeUBR9VzWdx9BjADIB6Pe4NV3tje/HUQFIPPhXN+Du27ZboiEZEqsiJc9yJgoJkVmlkuwQnrudXarAVOAzCzwUAesDlsN9nM2phZITAQWBhhrZmz8D7483/A4C/BxQ8oKESkSYrsyMLdK8xsGvACwWWxD7j7UjO7Eyh297nAN4D7zOwWgpPdU93dgaVm9gTByfAK4IYWeSXUovvhuW/CsWfDRQ+oy0lEmiwL9s3NXzwe9+Li4kyXUX+LZ8IzX4dBk+DLD0N2bqYrEpFWyMwWu3s8Xbsou6GkNm8/HATFwDPgyw8pKESkyVNYNLYlj8LcG+GY08IjCt1UJyJNn8KiMb37OMy5Ho4+BSY/oof8iUizobBoLO/NhjnXQeF4mPwo5LTNdEUiIvWmsGgM7/8R/ngNDBgHU2bp7XQi0uwoLKK2dA48+TXod2IYFO0zXZGIyCFTWERp+TPw5Feh7wlw2RPQpkOmKxIROSwKi6h88Bz8YSr0HgWX/QHadMx0RSIih01hEYUVf4YnroBeRXD5bMjrlH4ZEZEmTGHR0P7xIjzxL/CZoXD5HyGvc6YrEhE5YgqLhrTyJZh1GXQ/Dv7lKWibn+mKREQahMKioXz0ahAURw2CK54O3o8tItJCKCwawqq/wmNToOsxQVC065rpikREGpTC4kit+Rs8Nhm6FMCVc/U+ChFpkRQWR+LjN+CRL0PnfmFQHJXpikREIqGwOFxr34RHLoFOveHKZ6BDj0xXJCISGYXF4Vi3CH5/MXToGQRFx56ZrkhEJFKRhoWZTTKzFWa20sxurWH+z8xsSfj50My2p8z7kZktNbPlZvYLM7Moa623ksXw+wuhQ3eY+ifo1CvTFYmIRC6yd3CbWQyYDkwESoBFZjbX3Zftb+Put6S0vxEYGQ5/DhgHDA9n/w2YALwWVb318sk78PAFwdVOV/4p6IISEWkFojyyGAOsdPdV7l4GzALOq6P9FOCxcNiBPCAXaAPkABsjrDW9De/CQ+dD285BUHTuk9FyREQaU5Rh0QdYlzJeEk47iJkNAAqBVwDcfQHwKrAh/Lzg7ssjrLVu/+89eOi84GGAV/4J8vtlrBQRkUxoKie4JwOz3T0BYGafBQYDfQkC5gtmNr76QmZ2rZkVm1nx5s2bo6ls41L43bmQ0z44md1lQDTfIyLShEUZFuuB1D/B+4bTajKZA11QABcAb7r7LnffBTwPnFR9IXef4e5xd4937969gcpOsWl5EBTZeTD1Geha2PDfISLSDEQZFouAgWZWaGa5BIEwt3ojMzsO6AIsSJm8FphgZtlmlkNwcrtxu6E2fQC/+xJkZQdXPXU9ulG/XkSkKYksLNy9ApgGvECwo3/C3Zea2Z1mdm5K08nALHf3lGmzgY+A94B3gXfd/Zmoaj3I5g+DoLCsICi6HdNoXy0i0hRZ1X108xWPx724uPjIV/TpSph5NngCpj4L3Y898nWKiDRRZrbY3ePp2jWVE9xNw5aP4HfnQLIiOJmtoBARASK8Ka/Z2bo66HpKlAVB0WNwpisSEWkyFBYA2z4OgqJ8TxAUPY/PdEUiIk2KuqF2rA+6nvbtDF5c9Jlhma5IRKTJUVjkdYaew+CKOdCrKNPViIg0SeqGatMBpjya6SpERJo0HVmIiEhaCgsREUlLYSEiImkpLEREJC2FhYiIpKWwEBGRtBQWIiKSlu6zEJFIlZeXU1JSQmlpaaZLadXy8vLo27cvOTk5h7W8wkJEIlVSUkLHjh0pKCjAzDJdTqvk7mzZsoWSkhIKCw/vjZ/qhhKRSJWWltKtWzcFRQaZGd26dTuiozuFhYhETkGReUf63yDSsDCzSWa2wsxWmtmtNcz/mZktCT8fmtn2lHn9zewvZrbczJaZWUGUtYqISO0iCwsziwHTgbOAIcAUMxuS2sbdb3H3Ee4+Avgl8MeU2Q8BP3b3wcAYYFNUtYpIy/aLX/yCwYMHc9lllx3ScmvWrOHRR/WgUYj2yGIMsNLdV7l7GTALOK+O9lOAxwDCUMl29xcB3H2Xu++JsFYRacH+93//lxdffJFHHnnkkJY73LBIJBKHvExTF2VY9AHWpYyXhNMOYmYDgELglXDSIGC7mf3RzN4xsx+HRyoiIofkuuuuY9WqVZx11lncfffdXH311YwZM4aRI0fy9NNPA0EojB8/nlGjRjFq1CjeeOMNAG699Vbmz5/PiBEj+NnPfsbMmTOZNm1a5brPOeccXnvtNQA6dOjAN77xDYqKiliwYAGLFy9mwoQJjB49mjPPPJMNGzYAwVHOkCFDGD58OJMnT27cjXEk3D2SD3Ax8NuU8X8B/m8tbf8D+GW1ZXcARxNc3vsk8NUalrsWKAaK+/fv7yLS9CxbtizTJfiAAQN88+bNftttt/nDDz/s7u7btm3zgQMH+q5du3z37t2+d+9ed3f/8MMPffTo0e7u/uqrr/rZZ59duZ4HH3zQb7jhhsrxs88+21999VV3dwf88ccfd3f3srIyP+mkk3zTpk3u7j5r1iy/6qqr3N29V69eXlpaWllDY6rpvwVQ7PXYp0d5n8V6oF/KeN9wWk0mAzekjJcAS9x9FYCZzQFOBO5PXcjdZwAzAOLxuDdM2SLSUv3lL39h7ty5/OQnPwGCy3rXrl1L7969mTZtGkuWLCEWi/Hhhx8e8rpjsRgXXXQRACtWrOD9999n4sSJQNAt1atXLwCGDx/OZZddxvnnn8/555/fQL9Z9KIMi0XAQDMrJAiJycBXqjcys+OALsCCasvmm1l3d98MfIHgCEJE5LC5O08++STHHntslel33HEHPXv25N133yWZTJKXl1fj8tnZ2SSTycrx1PsW8vLyiMVild9z/PHHs2DBgoPW8eyzzzJv3jyeeeYZ7r77bt577z2ys5v+/dGRnbNw9wpgGvACsBx4wt2XmtmdZnZuStPJwKzwcGj/sgngm8DLZvYeYMB9UdUqIq3DmWeeyS9/+cv93di88847AOzYsYNevXqRlZXFww8/XHmCumPHjuzcubNy+YKCApYsWUIymWTdunUsXLiwxu859thj2bx5c2VYlJeXs3Tp0srlTj31VH74wx+yY8cOdu3aFeWv3GAijTN3fw54rtq071Ubv6OWZV8EhkdWnIi0Ot/97ne5+eabGT58OMlkksLCQv70pz9x/fXXc9FFF/HQQw8xadIk2rdvDwRdRrFYjKKiIqZOncrNN99MYWEhQ4YMYfDgwYwaNarG78nNzWX27NncdNNN7Nixg4qKCm6++WYGDRrE5Zdfzo4dO3B3brrpJvLz8xtzExw2S/mDvlmLx+NeXKyeKpGmZvny5QwePDjTZQg1/7cws8XuHk+3rB73ISIiaSksREQkLYWFiIikVWtYmNmZZnZxDdMvNrOJ0ZYlIiJNSV1HFt8D/lrD9NeAOyOpRkREmqS6wqJNeENcFe7+KdA+upJERKSpqSssOpnZQfdhmFkO0Da6kkREGtaaNWsYOnRopsto1uoKiz8C95lZ5VGEmXUAfk3V906IiEgLV1dY3A5sBD42s8Vm9jawGtgczhMRaTYqKiq47LLLGDx4MBdffDF79uzh5ZdfZuTIkQwbNoyrr76affv2sWjRIi688EIAnn76adq2bUtZWRmlpaUcffTRta7/vvvu44QTTqCoqIiLLrqIPXuCV/BMnTqV2bNnV7br0KFD5fAPf/hDhg0bRlFREbfeetDLRJuUWh/3ET7b6VYz+0/gs+Hkle6+t1EqE5EW5z+fWcqyT/7ZoOsc0rsT3//S8WnbrVixgvvvv59x48Zx9dVX89Of/pTf/OY3vPzyywwaNIgrrriCX/3qV5VPnwWYP38+Q4cOZdGiRVRUVDB27Nha13/hhRdyzTXXAHD77bdz//33c+ONN9ba/vnnn+fpp5/mrbfeol27dmzduvUQf/PGVdelsxea2YUEr0UdSBAYcTPr2FjFiYg0lH79+jFu3DgALr/8cl5++WUKCwsZNGgQAFdeeSXz5s0jOzubY445huXLl7Nw4UL+7d/+jXnz5jF//nzGjx9f6/rff/99xo8fz7Bhw3jkkUdYunRpnfW89NJLXHXVVbRr1w6Arl27NtBvGo26HiT4pRqmdQWGm9lX3f2VGuaLiNSqPkcAUTGzKuP5+fls2bKlxrYnn3wyzz//PDk5OZx++ulMnTqVRCLBj3/841rXP3XqVObMmUNRUREzZ86sfINe6mPNk8kkZWVlDfMLNbJajyzc/aoaPucBpwD/3WgViog0gLVr11Y+MvzRRx8lHo+zZs0aVq5cCcDDDz/MhAkTABg/fjw///nPOemkk+jevTtbtmxhxYoVdV5RtXPnTnr16kV5eXmVd30XFBSwePFiAObOnUt5eTkAEydO5MEHH6w8t9Fsu6Fq4+4fAzkR1CIiEpljjz2W6dOnM3jwYLZt28Ytt9zCgw8+yCWXXMKwYcPIysriuuuuA2Ds2LFs3LiRk08+GQgeVT5s2LCDjk5S3XXXXYwdO5Zx48Zx3HHHVU6/5ppr+Otf/1r5bu79jz+fNGkS5557LvF4nBEjRlS+va+pOuRHlIdvtnvQ3U+KpqTDo0eUizRNekR503Ekjyiv9ZyFmT0DVE+SrkAv4PLDqFNERJqpuk5wVz8mcmArQWBcTtV3ZouItAo33HADr7/+epVpX//617nqqqsyVFHjqOs+i8qHCJrZSOArwCUEN+Y9WZ+Vm9kk4F4gBvzW3e+pNv9nwKnhaDugh7vnp8zvBCwD5rj7tPp8p4hIlKZPn57pEjKirm6oQcCU8PMp8DjBOY5Ta1um2vIxYDowESgBFpnZXHdftr+Nu9+S0v5GYGS11dwFzKvfryIiIlGp62qoD4AvAOe4++fd/ZdA4hDWPYbgju9V7l4GzALOq6P9FOCx/SNmNhroCfzlEL5TREQiUFdYXAhsAF41s/vM7DSg9uvGDtYHWJcyXhJOO4iZDQAKgVfC8Szgf4Bv1vUFZnatmRWbWfHmzQc9TV1ERBpIXTflzXH3ycBxwKvAzUAPM/uVmZ3RwHVMBma7+/4jl+uB59y9pK6F3H2Gu8fdPd69e/cGLklERPZLe1Oeu+9290fd/UtAX+Ad4D/qse71QL+U8b7htJpMJqULCjgJmGZmawiuyrrCzO6paUERkabijjvuaPI31x2uui6dPYi7bwNmhJ90FgEDzayQICQmE1xRVUV4k18XUi7FdffLUuZPBeLu3rSf3ysicogqKirIzj6k3XDGHPLjPuorfMT5NOAFYDnwhLsvNbM7zezclKaTgVl+qLeSi4jU0+9//3vGjBnDiBEj+Nd//VcSiQQdOnTgO9/5DkVFRZx44ols3LgRgI0bN3LBBRdQVFREUVERb7zxBgA//elPGTp0KEOHDuXnP/955brvvvtuBg0axOc//3lWrFhROf2jjz5i0qRJjB49mvHjx/PBBx8AwQMHr7vuOsaOHcu3vvWtGutduHAhJ510EiNHjuRzn/tc5XpnzpzJtGkH7iI455xzKh9Y+Oc//5lRo0ZRVFTEaaed1nAbLxRppLn7c8Bz1aZ9r9r4HWnWMROY2cCliUgmPH8r/L/3GnadnxkGZ9XeS718+XIef/xxXn/9dXJycrj++ut55JFH2L17NyeeeCJ333033/rWt7jvvvu4/fbbuemmm5gwYQJPPfUUiUSCXbt2sXjxYh588EHeeust3J2xY8cyYcIEkskks2bNYsmSJVRUVDBq1ChGjx4NwLXXXsuvf/1rBg4cyFtvvcX111/PK68ED+suKSnhjTfeIBaL1Vjzcccdx/z588nOzuall17i29/+Nk8+WfvtbZs3b+aaa65h3rx5FBYWRvJQwuZx/CMicphefvllFi9ezAknnADA3r176dGjB7m5uZxzzjkAjB49mhdffBGAV155hYceegiAWCxG586d+dvf/sYFF1xQ+RDACy+8kPnz55NMJrngggsq30lx7rlBp8muXbt44403uOSSSyrr2LdvX+XwJZdcUmtQAOzYsYMrr7ySf/zjH5hZ5ZNqa/Pmm29y8sknU1hYCETzbgyFhYg0njqOAKLi7lx55ZX8939XfbPCT37yk8qnyMZiMSoqKhrsO5PJJPn5+ZVv3Ktuf+jU5rvf/S6nnnoqTz31FGvWrOGUU04Bqr4bA6C0tLTBak4nsnMWIiJNwWmnncbs2bPZtGkTELw34uOPP66z/a9+9SsAEokEO3bsYPz48cyZM4c9e/awe/dunnrqKcaPH8/JJ5/MnDlz2Lt3Lzt37uSZZ54BoFOnThQWFvKHP/wBCALr3XffrXfNO3bsoE+f4La0mTNnVk4vKChgyZIlJJNJ1q1bx8KFCwE48cQTmTdvHqtXr678HRuawkJEWrQhQ4bwgx/8gDPOOIPhw4czceJENmzYUGv7e++9l1dffZVhw4YxevRoli1bxqhRo5g6dSpjxoxh7NixfO1rX2PkyJGMGjWKSy+9lKKiIs4666zKri6ARx55hPvvv5+ioiKOP/54nn766XrX/K1vfYvbbruNkSNHVjniGTduHIWFhQwZMoSbbrqJUaNGAdC9e3dmzJjBhRdeSFFREZdeeulhbKm6HfL7LJoqvc9CpGnS+yyajiN5n4WOLEREJC2d4BYRyZAHH3yQe++9t8q0cePGNcnHoCssREQy5Kqrrmo2L01SN5SIRK6lnBttzo70v4HCQkQilZeXx5YtWxQYGeTubNmyhby8vMNeh7qhRCRSffv2paSkBL1zJrPy8vLo27fvYS+vsBCRSOXk5FQ+hkKaL3VDiYhIWgoLERFJS2EhIiJpKSxERCQthYWIiKQVaViY2SQzW2FmK83soHdom9nPzGxJ+PnQzLaH00eY2QIzW2pmfzezhn+EooiI1Ftkl86aWQyYDkwESoBFZjbX3Zftb+Put6S0vxEYGY7uAa5w93+YWW9gsZm94O7bo6pXRERqF+WRxRhgpbuvcvcyYBZwXh3tpwCPAbj7h+7+j3D4E2AT0D3CWkVEpA5RhkUfYF3KeEk47SBmNgAoBF6pYd4YIBf4qIZ515pZsZkV6+5QEZHoNJUT3JOB2e6eSJ1oZr2Ah4Gr3D1ZfSF3n+HucXePd++uAw8RkahEGRbrgX4p433DaTWZTNgFtZ+ZdQKeBb7j7m9GUqGIiNRLlGGxCBhoZoVmlksQCHOrNzKz44AuwIKUabnAU8BD7j47whpFRKQeIgsLd68ApgEvAMuBJ9x9qZndaWbnpjSdDMzyqs8v/jJwMjA15dLaEVHVKiIidbOW8oz5eDzuxcXFmS5DRKRZMbPF7h5P166pnOAWEZEmTGEhIiJpKSxERCQthYWIiKSlsBARkbQUFiIikpbCQkRE0lJYiIhIWgoLERFJS2EhIiJpKSxERCQthYWIiKSlsBARkbQUFiIiklZ2pgsQEWlI7k55wimtSFBanmBfeZJ9FQlKy5PBeEXws7S26RVVl0ltU335pDt5ObHwk0Vedoy2ueHw/unZMdrmHpjXJidGXnZW0C6lfZvK4RhtU9aXlWWZ3qSAwkKkWdhblmBveYKKRJLypAc/E04i6ZQnklSE04KfTnkySUXCSSSDdhXJA+33L1uRPNB+/3or11dtmQPTHHBSX4OzfzD13TgHptXdjmrtHD94Wg3zkg77atnx76tIkDyC1/TkZmeRl50V7NTDHXab8Ge73Gy6tg927G1ysoiZUVpxIEhKyxNs2lleJVxKy5PsLU+QOMyi9tezP0jysmPk5VYNnIE9O/CNM449/F+6HhQWIhmWTDqf7t7H+m17+WR7KZ9s38v68PNJ+Nm2pzzyOnJiRnZWFtlZRnbMyI5lkZNlxGJGTlYW2TEjlpVF6h+6Fg4bVmU8mFatUcq0A8ulrstqmFZ1/ak/8tvl0iY7q/Kv+jbZVf+ib7N/h5/yc//01KOB1OVyY1mR/SVfnggCZG94tLO3WpikBs7eskRlCFW2L0tUHi3tDcNo6+4ySssTxBrh6CPSsDCzScC9QAz4rbvfU23+z4BTw9F2QA93zw/nXQncHs77gbv/LspaRaJSWp6oDIDgZxgI2/byyY69bNheSlkiWWWZ9rkx+nRpS5/8tozol0/v/La0z40FO/D9O/WUn1V39OG0rCxiWeG8WEoIZGVVnZZlxLKscmct0ciJZZETy6JjXk6mSzkskYWFmcWA6cBEoAR1IxY4AAAOhElEQVRYZGZz3X3Z/jbufktK+xuBkeFwV+D7QJzg6HVxuOy2qOoVORzuzpbdZZU7/yAQSquEw5bdZVWWMYOeHfPo06Utw/vmM2loHn3yg2DoHX465WVr5y1NSpRHFmOAle6+CsDMZgHnActqaT+FICAAzgRedPet4bIvApOAxyKsV6RSMumUJZLsq0iyLQyDkpRuoU+2l1aGwb6KqkcF7XJjlTv+oX060yc/CIbenYNpn+mcR05MFyJK8xJlWPQB1qWMlwBja2poZgOAQuCVOpbtE0GN0kRt3rmPtVv3UFaRpDyRPPAzHC5LJCnf/zPh7KveruJA2/KwTfVpVds6ZRWJoF0iWevJSDPo0bENvfPbMqR3JyYO6ZlyRBAcIXRum6OjAmlxmsoJ7snAbHdPHMpCZnYtcC1A//79o6hLGoG7s/rT3RSv2caiNVsp/ngbqz/dfcjryQ3783Ozg77h3OwscsOfOZU/jU5tc8hNbRfLIielbW6savv8drmV3UQ9O7ehTXYsgq0g0rRFGRbrgX4p433DaTWZDNxQbdlTqi37WvWF3H0GMAMgHo8fwcVy0pjKE0mWffLPIBjWbKP44618uivo1+/SLod4QVemjOnHwJ4dycuOkZtt5MZi5GRb5Y68TZUACHbq+mteJDpRhsUiYKCZFRLs/CcDX6neyMyOA7oAC1ImvwD8l5l1CcfPAG6LsFaJ0O59FbyzdjuL1mxl0ZqtvLN2O3vLg4PI/l3bcfKg7owp6Eq8oCvHdG+vnb5IExRZWLh7hZlNI9jxx4AH3H2pmd0JFLv73LDpZGCWp9yp4+5bzewugsABuHP/yW5p+jbtLGXxmm0sDI8clm34J4mkk2UwuFcnLj2hHycUdCVe0IWenfIyXa6I1IPVdDdlcxSPx724uDjTZbQ6qecbgnDYypotewDIy8liZL8unFDQhXhBV0b2z2+215iLtFRmttjd4+naNZUT3NJMpJ5v2H/OYf99BF3b5xIf0IXLxg4gXtCF43t3Jjdbl4iKtAQKC6nT7n0VvL12G4vWbKO42vmGAd3accqxPSqPHHS+QaTlUlhIFZt2lh64hLXa+YYhvYPzDWMKuxIf0IUeOt8g0mooLFqx+pxvuOGUYzihsCsj+3ehQxv9cxFprfR/fytSkUiybMM/Wbj64Psbqp9vGNqnsx5JISKVFBYt2J6yCpas3V55Cevba7exp0z3N4jIoVNYtCBbdu2rPBG96ONtLF2/g4qkYwaDP9OJS0b35YTCrsQHdOUznXW+QUTqT2HRTLk767burTzXsHDNVlZtDp6nlJudxYh++Vw34RjiBV0YNaALnXR/g4gcAYVFM5FIOss3/DM4agivVtq0cx8AndvmEB/QhS/H+3FCeL5BD7sTkYaksGiiSssTLFm3PTxq2MbbH29j174KAPrkt+Vzx3QjXtCVMYVd+Wz3Dk3mpe4i0jIpLJqIRNL564ebeGv1Vhat3sp763dQngjONxzbsyPnj+wdPk+pK33y22a6XBFpZRQWTcCSddv57pz3eW/9DnJjWQzv25mvfv5oxhR2YXT/rnRup/MNIpJZCosM2r6njB+9sILHFq6le4c23Dt5BGce/xnycnS+QUSaFoVFBiSTzuy3S7jn+Q/Ysbecq8cVcvPpA/VEVhFpshQWjWzZJ//ku0+/z+KPtxEf0IW7zh/K4F6dMl2WiEidFBaNZGdpOT978R/8bsEaOrfN4ccXD+eiUX11FZOINAsKi4i5O8/8fQM/+NMyNu/ax1fG9OffzzyW/Ha5mS5NRKTeIg0LM5sE3EvwWtXfuvs9NbT5MnAH4MC77v6VcPqPgLOBLOBF4OvezF7rt3LTLr739Pu88dEWhvXpzH1XxCnql5/pskREDllkYWFmMWA6MBEoARaZ2Vx3X5bSZiBwGzDO3beZWY9w+ueAccDwsOnfgAnAa1HV25D2liX45Sv/4L75q8jLiXHX+UP5ypj+xNTlJCLNVJRHFmOAle6+CsDMZgHnActS2lwDTHf3bQDuvimc7kAekAsYkANsjLDWBuHuvLhsI//5zDLWb9/LhaP6cNtZg+nesU2mSxMROSJRhkUfYF3KeAkwtlqbQQBm9jpBV9Ud7v5nd19gZq8CGwjC4v+6+/IIaz1i67bu4Y65S3n5g00M6tmBx689kbFHd8t0WSIiDSLTJ7izgYHAKUBfYJ6ZDQOOAgaH0wBeNLPx7j4/dWEzuxa4FqB///6NVXMV+yoS/Oavq5j+6kqys4zvfHEwU8cV6MVBItKiRBkW64F+KeN9w2mpSoC33L0cWG1mH3IgPN50910AZvY8cBJQJSzcfQYwAyAejzf6ye95H27m+3OXsvrT3Zw9rBe3nzOYXp313CYRaXmi/PN3ETDQzArNLBeYDMyt1mYOQTBgZkcRdEutAtYCE8ws28xyCE5uN5luqA079nLDI29zxQMLAXjo6jFMv2yUgkJEWqzIjizcvcLMpgEvEJyPeMDdl5rZnUCxu88N551hZsuABPDv7r7FzGYDXwDeIzjZ/Wd3fyaqWuurPJHkwddX8/OX/kEi6Xxj4iCunXC03h0hIi2eNbNbF2oVj8e9uLg4svUvXL2V2+e8x4cbd/GF43rwn+ceT7+u7SL7PhGRxmBmi909nq5dpk9wN3mbd+7jv59fzh/fXk+f/LbM+JfRTBzSEzPdMyEirYfCohaJpPPoWx/zoxdWUFqe4IZTj2HaqQNpm6suJxFpfRQWNXh33XZuD19G9LljunHneUP5bI8OmS5LRCRjFBYptu8p48cvrODR8GVEv5gyki8N76UuJxFp9RQWHPwyoqs+V8gtE/UyIhGR/Vp9WHyyfS83PfYOxR9vY/SALtx13lCG9NbLiEREUrX6sOjSLpeyRJIfXTSci0frZUQiIjVp9WHRNjfG0zeM03kJEZE66Gl3oKAQEUlDYSEiImkpLEREJC2FhYiIpKWwEBGRtBQWIiKSlsJCRETSUliIiEhaLeblR2a2Gfg403UcoaOATzNdRBOi7VGVtscB2hZVHcn2GODu3dM1ajFh0RKYWXF93ljVWmh7VKXtcYC2RVWNsT3UDSUiImkpLEREJC2FRdMyI9MFNDHaHlVpexygbVFV5NtD5yxERCQtHVmIiEhaCosMMLNJZrbCzFaa2a01zP83M1tmZn83s5fNbEAm6mws6bZHSruLzMzNrMVeBVOfbWFmXw7/fSw1s0cbu8bGVI//V/qb2atm9k74/8sXM1FnYzCzB8xsk5m9X8t8M7NfhNvq72Y2qkELcHd9GvEDxICPgKOBXOBdYEi1NqcC7cLh/wM8num6M7k9wnYdgXnAm0A803Vn8N/GQOAdoEs43iPTdWd4e8wA/k84PARYk+m6I9weJwOjgPdrmf9F4HnAgBOBtxry+3Vk0fjGACvdfZW7lwGzgPNSG7j7q+6+Jxx9E+jbyDU2prTbI3QX8EOgtDGLa2T12RbXANPdfRuAu29q5BobU322hwOdwuHOwCeNWF+jcvd5wNY6mpwHPOSBN4F8M+vVUN+vsGh8fYB1KeMl4bTafJXgr4WWKu32CA+n+7n7s41ZWAbU59/GIGCQmb1uZm+a2aRGq67x1Wd73AFcbmYlwHPAjY1TWpN0qPuWQ9Lq38HdlJnZ5UAcmJDpWjLFzLKAnwJTM1xKU5FN0BV1CsER5zwzG+bu2zNaVeZMAWa6+/+Y2UnAw2Y21N2TmS6spdGRReNbD/RLGe8bTqvCzE4HvgOc6+77Gqm2TEi3PToCQ4HXzGwNQV/s3BZ6krs+/zZKgLnuXu7uq4EPCcKjJarP9vgq8ASAuy8A8giek9Qa1WvfcrgUFo1vETDQzArNLBeYDMxNbWBmI4HfEARFS+6ThjTbw913uPtR7l7g7gUE53DOdffizJQbqbT/NoA5BEcVmNlRBN1SqxqzyEZUn+2xFjgNwMwGE4TF5katsumYC1wRXhV1IrDD3Tc01MrVDdXI3L3CzKYBLxBc7fGAuy81szuBYnefC/wY6AD8wcwA1rr7uRkrOkL13B6tQj23xQvAGWa2DEgA/+7uWzJXdXTquT2+AdxnZrcQnOye6uGlQS2NmT1G8IfCUeE5mu8DOQDu/muCczZfBFYCe4CrGvT7W+h2FRGRBqRuKBERSUthISIiaSksREQkLYWFiIikpbAQEZG0FBYiIpKWwkJaPTPLN7PrD2O558wsP4qaRJoa3WchrZ6ZFQB/cveh1aZnu3tFRoqqRVOsSVoHHVmIwD3AMWa2xMwWmdl8M5sLLAMwszlmtjh82dC1+xcyszVmdpSZFZjZcjO7L2zzFzNrW9uXmdlrZnZv+H3vm9mYcHr78AU3C8OX+ZwXTp9qZnPN7BXg5XDaf5jZe2b2rpndE+G2EQH0uA8RgFuBoe4+wsxOAZ4Nx1eH8692961hACwysydreMTGQGCKu19jZk8AFwG/r+M724XfdzLwAMHDEr8DvOLuV4fdWwvN7KWw/ShgeFjHWQTvLhjr7nvMrOuRbwKRuiksRA62MCUoAG4yswvC4X4EwVA9LFa7+5JweDFQkOY7HoPghTZm1ikMhzOAc83sm2GbPKB/OPyiu+9/8c3pwIP7X5CVMl0kMgoLkYPt3j8QHmmcDpwU/hX/GsFOvLrUx8gngFq7oULVTxY6weswL3L3FakzzGxsak0imaBzFiKwk+C9GTXpDGwLg+I4gvdpNIRLAczs8wSPkt5B8HTVGy181HD4qPqavAhcZWbtwnbqhpLI6chCWj133xK+pvR9YC+wMWX2n4HrzGw5sILgfRoNodTM3iF4xPTV4bS7gJ8Dfw/fELgaOKeGev9sZiOAYjMrI3g09bcbqC6RGunSWZFGFnZlfbOFvsBJWih1Q4mISFrqhhKJiJlNB8ZVm3yvu5+SgXJEjoi6oUREJC11Q4mISFoKCxERSUthISIiaSksREQkLYWFiIik9f8Bz/BA0iQUWT0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "results_df = pd.DataFrame({\"train_perc\": train_sizes,\n",
    "                           \"bow_auc\": np.mean(g_test_scores, axis=1),\n",
    "                           \"encoder_auc\": np.mean(e_test_scores, axis=1)})\n",
    "\n",
    "sns.lineplot(x=\"train_perc\", y=\"AUC\", hue=\"features\", \n",
    "             data=results_df.melt(\"train_perc\", var_name=\"features\", value_name=\"AUC\"), \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder features outperform the bag-of-words (BoW) glove vectors at every level of training data experience. While a more careful aggregation procedure of the word vectors would have done much better (in fact, there's good evidence that a thoughtful weighted-average can be [very hard to beat](https://openreview.net/forum?id=SyK00v5xx) on many discriminative tasks), the main point of this analysis is that using pre-trained encoders can basically be a drop-in replacement for word vectors for many applications and give significant gains, _modulo_ additional computation time to featurize the dataset (the BoW approach uses a lookup to compute features, which is very fast, whereas the encoder approach requires a full forward pass through a complicated recurrent neural network, which are inherently sequential (this is why there is a greater push towards [feed-forward architectures for language modeling](https://blog.openai.com/language-unsupervised/), which can be much faster during training and evaluation time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis __WARNING: FOUL LANGUAGE AHEAD__:\n",
    "\n",
    "Let's see some examples of where our model using language model features outperformed our BoW model. We'll be a bit more fair to both models this time and do a quick grid search to find a well-optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/tensorflow/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel/__main__.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/envs/tensorflow/lib/python3.6/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "model_df = toxic_df.loc[:,['is_attack', 'comment_text', 'glove_aggregate', 'encoded_comment']]\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "train_df, test_df = train_test_split(model_df, train_size=0.75)\n",
    "\n",
    "def cv_predict_eval():\n",
    "\n",
    "    cv = GridSearchCV(RandomForestClassifier(),\n",
    "                     param_grid={\n",
    "                         'n_estimators': [10, 100],\n",
    "                         'max_features': ['sqrt', 'log2'],\n",
    "                         'max_depth': [3, 5, None]}, \n",
    "                      refit=True, \n",
    "                      n_jobs=20)\n",
    "\n",
    "\n",
    "    labels, glove_features, use_features = featurize(train_df)\n",
    "    labels_test, glove_test, use_test = featurize(test_df)\n",
    "\n",
    "    glove_cv = cv.fit(glove_features, labels)\n",
    "    glove_hat = cv.predict(glove_test)\n",
    "    use_cv = cv.fit(use_features, labels)\n",
    "    use_hat = cv.predict(use_test)\n",
    "\n",
    "    results_df = test_df\n",
    "    results_df['use_pred'] = use_hat\n",
    "    results_df['glove_pred'] = glove_hat\n",
    "\n",
    "    return results_df\n",
    "\n",
    "results_df = cv_predict_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where BoW-GloVe Fails and the Encoder Succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>is_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26284</th>\n",
       "      <td>See http www.stella doro.com contact.asp</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91167</th>\n",
       "      <td>sound right coming out of your mouth</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23154</th>\n",
       "      <td>BUT...BUT...THAT WAS THEIR REACTION</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115045</th>\n",
       "      <td>on Rogers Communications</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44226</th>\n",
       "      <td>You think you re smarter than NATO or Romanian...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38194</th>\n",
       "      <td>Urrrppp   You wanna have my love Burp</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101589</th>\n",
       "      <td>Rorquals oh sorry, I don t think you know w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93592</th>\n",
       "      <td>cAN YOU BAN ME  well, can you</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86609</th>\n",
       "      <td>MY NAME IS ADAM SMITH AND I AM VERY LIKE A GO...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78974</th>\n",
       "      <td>Why do you keep reverting a perfectly reaso...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39925</th>\n",
       "      <td>dead  nice one dude</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8816</th>\n",
       "      <td>okay you guys i m really depressed so pleas...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11023</th>\n",
       "      <td>Why don t you give the Wikipedia community a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38494</th>\n",
       "      <td>GNU Free Documentation Licensed</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57977</th>\n",
       "      <td>YOU RE WELCOME, I WILL CONTINUE TO BE SINCER...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39675</th>\n",
       "      <td>You have no shame, you should admit that y...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92415</th>\n",
       "      <td>THIS IS MY PAGE YOU HAVE NO RIGHTS tO DELETE ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112872</th>\n",
       "      <td>Your pretty special you know love all you gu...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10323</th>\n",
       "      <td>Cherokee Language Wikipedia</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113330</th>\n",
       "      <td>HE SHOULD HAVE A PAGE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61796</th>\n",
       "      <td>ITDTH im very sorry about everything and wha...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26000</th>\n",
       "      <td>are you a message machine or somthing   he...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41485</th>\n",
       "      <td>Next person sexy boy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53184</th>\n",
       "      <td>Yeah, getting kinda lazy in my old age email</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51391</th>\n",
       "      <td>User, YOU ARE MISTAKEN. THE CAPITAL HILL BLU...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75229</th>\n",
       "      <td>Wake up and smell the coffee...remember Le...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34831</th>\n",
       "      <td>wanna blank this cuz i m liberal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53864</th>\n",
       "      <td>AND WHY AND HOW IS IT AND CAN IT BE THAT THE...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78633</th>\n",
       "      <td>You are harrassing me, then.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94192</th>\n",
       "      <td>THE ARTICLE TITLED THE ELLIOTT ARGUMENT IS ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107317</th>\n",
       "      <td>dung. Libra, aren t you</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63086</th>\n",
       "      <td>Word</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61433</th>\n",
       "      <td>hi po musta na po kayo by the way im ralph gui...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27650</th>\n",
       "      <td>City infobox  Why can t this city have a re...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13768</th>\n",
       "      <td>DONT MESS WITH AAAAAAAAAAAAAAAAAAAAAAAAAAAAA...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77451</th>\n",
       "      <td>The usual bored and horny...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37424</th>\n",
       "      <td>DON T FORGET...   To tell your poppa and mo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95099</th>\n",
       "      <td>BUT I DIDNT DISCLOSED PERSONAL INFORMATION A...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56600</th>\n",
       "      <td>lol   I think I love you...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66166</th>\n",
       "      <td>You just don t have a clue, do you go and re...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96769</th>\n",
       "      <td>Edit War   There are some mighty busy admin...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53078</th>\n",
       "      <td>its amazing to yo momma</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44719</th>\n",
       "      <td>CAN T HAVE MY POPCORN FRO</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110993</th>\n",
       "      <td>Na na na na, na na na na, hey hey hey, goodb...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46468</th>\n",
       "      <td>ive found alot of vandalism can somone fix it</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32213</th>\n",
       "      <td>Bold text WHY ARE YOU DELECTING MY ADD AT THE ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87567</th>\n",
       "      <td>Why do you delete my comments It s a good fr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16478</th>\n",
       "      <td>this is for you my dear Ann</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43515</th>\n",
       "      <td>a maniac maniac on da floor</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47357</th>\n",
       "      <td>You get stuffed too.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43526</th>\n",
       "      <td>STOP IT   STOP DELETING STUFF I AM FIXING I...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37146</th>\n",
       "      <td>SANDAHL IS OF QUESTIONABLE ETHNICITY AND I HA...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32393</th>\n",
       "      <td>GIANO AND GUSTAV MAKE UP HISTORY AND IGNORE VA...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52510</th>\n",
       "      <td>hey   do u like the page good stuff huh lol...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15252</th>\n",
       "      <td>You know what the fun thing about Wikipedia i...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105512</th>\n",
       "      <td>thanks a lot man, you dont know how long thats...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24772</th>\n",
       "      <td>ACCESS DENEID   ACCESS DENEID ACCESS DENEID...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47831</th>\n",
       "      <td>Chuck Norris the coolest guy ever</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19848</th>\n",
       "      <td>like a fat kid loves cake</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77442</th>\n",
       "      <td>why you cleen my mesage maybe i help you. gi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  is_attack\n",
       "26284            See http www.stella doro.com contact.asp      False\n",
       "91167                sound right coming out of your mouth      False\n",
       "23154                BUT...BUT...THAT WAS THEIR REACTION       False\n",
       "115045                           on Rogers Communications      False\n",
       "44226   You think you re smarter than NATO or Romanian...      False\n",
       "38194              Urrrppp   You wanna have my love Burp       False\n",
       "101589     Rorquals oh sorry, I don t think you know w...      False\n",
       "93592                      cAN YOU BAN ME  well, can you       False\n",
       "86609    MY NAME IS ADAM SMITH AND I AM VERY LIKE A GO...      False\n",
       "78974      Why do you keep reverting a perfectly reaso...      False\n",
       "39925                                dead  nice one dude       False\n",
       "8816       okay you guys i m really depressed so pleas...      False\n",
       "11023     Why don t you give the Wikipedia community a...      False\n",
       "38494                     GNU Free Documentation Licensed      False\n",
       "57977     YOU RE WELCOME, I WILL CONTINUE TO BE SINCER...      False\n",
       "39675       You have no shame, you should admit that y...      False\n",
       "92415    THIS IS MY PAGE YOU HAVE NO RIGHTS tO DELETE ...      False\n",
       "112872    Your pretty special you know love all you gu...      False\n",
       "10323                         Cherokee Language Wikipedia      False\n",
       "113330                              HE SHOULD HAVE A PAGE      False\n",
       "61796     ITDTH im very sorry about everything and wha...      False\n",
       "26000       are you a message machine or somthing   he...      False\n",
       "41485                              Next person sexy boy        False\n",
       "53184       Yeah, getting kinda lazy in my old age email       False\n",
       "51391     User, YOU ARE MISTAKEN. THE CAPITAL HILL BLU...      False\n",
       "75229       Wake up and smell the coffee...remember Le...      False\n",
       "34831                    wanna blank this cuz i m liberal      False\n",
       "53864     AND WHY AND HOW IS IT AND CAN IT BE THAT THE...      False\n",
       "78633                        You are harrassing me, then.      False\n",
       "94192      THE ARTICLE TITLED THE ELLIOTT ARGUMENT IS ...      False\n",
       "...                                                   ...        ...\n",
       "107317                          dung. Libra, aren t you        False\n",
       "63086                                               Word       False\n",
       "61433   hi po musta na po kayo by the way im ralph gui...      False\n",
       "27650      City infobox  Why can t this city have a re...      False\n",
       "13768     DONT MESS WITH AAAAAAAAAAAAAAAAAAAAAAAAAAAAA...      False\n",
       "77451                        The usual bored and horny...      False\n",
       "37424      DON T FORGET...   To tell your poppa and mo...      False\n",
       "95099     BUT I DIDNT DISCLOSED PERSONAL INFORMATION A...      False\n",
       "56600                         lol   I think I love you...      False\n",
       "66166     You just don t have a clue, do you go and re...      False\n",
       "96769      Edit War   There are some mighty busy admin...      False\n",
       "53078                             its amazing to yo momma      False\n",
       "44719                           CAN T HAVE MY POPCORN FRO      False\n",
       "110993    Na na na na, na na na na, hey hey hey, goodb...      False\n",
       "46468      ive found alot of vandalism can somone fix it       False\n",
       "32213   Bold text WHY ARE YOU DELECTING MY ADD AT THE ...      False\n",
       "87567     Why do you delete my comments It s a good fr...      False\n",
       "16478                         this is for you my dear Ann      False\n",
       "43515                         a maniac maniac on da floor      False\n",
       "47357                                You get stuffed too.      False\n",
       "43526      STOP IT   STOP DELETING STUFF I AM FIXING I...      False\n",
       "37146    SANDAHL IS OF QUESTIONABLE ETHNICITY AND I HA...      False\n",
       "32393   GIANO AND GUSTAV MAKE UP HISTORY AND IGNORE VA...      False\n",
       "52510      hey   do u like the page good stuff huh lol...      False\n",
       "15252    You know what the fun thing about Wikipedia i...      False\n",
       "105512  thanks a lot man, you dont know how long thats...      False\n",
       "24772      ACCESS DENEID   ACCESS DENEID ACCESS DENEID...      False\n",
       "47831                  Chuck Norris the coolest guy ever       False\n",
       "19848                          like a fat kid loves cake       False\n",
       "77442     why you cleen my mesage maybe i help you. gi...      False\n",
       "\n",
       "[117 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[(results_df[\"is_attack\"] == results_df[\"use_pred\"]) & \n",
    "               (results_df[\"is_attack\"] != results_df[\"glove_pred\"]) & \n",
    "               (results_df[\"is_attack\"] == False), \n",
    "               [\"comment_text\", \"is_attack\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' ok ...sorry thougt it was fun wont do it again ill use the dumb sandbox or w e'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[[25534], \"comment_text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's unfortunately pretty challenging to find any civil discussion on online forums. In the example highlighted above our BoW approach predicted the comment to be an attack, whereas the encoder correctly predicted it as benign. The prevalence of terms such \"dumb\", \"sorry\" _bias_ the average word vector to a representation that is more likely to be an attack than a nice comment, whereas the language model is able to encode the sequence of representations more accurately. A more acute example of this phenomena arises with swear words (ommitted here, but see for example record index `[105512]` for an example where the single occurrence of a curse word causes the BoW classifier to misclassify the sentiment dramatically (but the encoder gets it right). This also arises frequently in the use of **negation** in sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
