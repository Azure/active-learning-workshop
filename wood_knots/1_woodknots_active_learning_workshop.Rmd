---
title: "Active Learning for Image Classification"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=FALSE, message=FALSE, warning=FALSE, fig.height=7.5)
rxOptions(reportProgress=0)
```

# Classifying wood knots

This is a followup to our earlier blog post "[Featurizing images: the shallow end of deep learning](http://blog.revolutionanalytics.com/2017/09/wood-knots.html#more)". That article contains the code for generating the features for the training and test datasets, which were saved to a csv file. Here we begin by loading that file.


```{r parameters}

### Libraries ###
library(dplyr)
library(tidyr)
library(ggplot2)
library(reticulate)
library(pROC)

### Meta-hyperparameters ###
set.seed(3)  ###

L1_PENALTY <- 1e-2
L2_PENALTY <- 1e-2

INITIAL_EXAMPLES_PER_CLASS <- 6  # cases from the labelled dataset used to train the initial model
ADDITIONAL_CASES_TO_LABEL <- 12  # cases to label per iteration
NUM_ITERATIONS <- 15
MONTE_CARLO_SAMPLES <- 100

# This order determines the order of factor levels
KNOT_CLASSES <- setNames(nm=c("sound_knot", "dry_knot", "encased_knot"))

LABELLED_FEATURIZED_DATA <- "data/labelled_knots_featurized_resnet18.Rds"
UNLABELLED_FEATURIZED_DATA <- "data/unlabelled_knots_featurized_large_resnet18.Rds"

unlabelled_knot_data_df <- readRDS(UNLABELLED_FEATURIZED_DATA)
labelled_knot_data_df <- readRDS(LABELLED_FEATURIZED_DATA)

# We'll pretend these come from our labellers
PSEUDOLABELS_FILE <- "data/unlabelled_knot_info_area_small.csv" 

inputs <- grep("^Feature", names(labelled_knot_data_df), value=TRUE)
outcome <- "knot_class"
FORM <- formula(paste(outcome, paste(inputs, collapse="+"), sep="~"))

```


### Split labelled data into training and test sets

```{r split_train_and_test_sets}

class_samples <- lapply(KNOT_CLASSES, function(kc) sample(which(labelled_knot_data_df$knot_class == kc), INITIAL_EXAMPLES_PER_CLASS))

in_training_set <- (1:nrow(labelled_knot_data_df) %in% unlist(class_samples))

initial_training_set <- labelled_knot_data_df[in_training_set,]
TEST_SET <- labelled_knot_data_df[!in_training_set,]

table(initial_training_set$knot_class)

table(TEST_SET$knot_class)
table(TEST_SET$knot_class)/nrow(TEST_SET)
```


## Initial model for knot classes

First we build a model on the available training data, and test on the test data. This model will focus on classifying a knot image into three categories: "sound_knot", "dry_knot", and "encased_knot". In the blog post there was also a category for "decayed_knot", but we will consider decay a separate attribute, and not use it in this model. We will use only a small number of the available labelled cases for training, and the rest for testing. This makes a larger test set and a smaller training set than in the blog post.

### Fit model to initial training set

```{r tune_and_train_model}
source("woodknots_active_learning_lib.R")

pseudolabel_function <- get_pseudolabelling_function(PSEUDOLABELS_FILE, KNOT_CLASSES)

initial_model_results <- fit_and_evaluate_model(initial_training_set)

```

## Results for initial model


### ROC curves

```{r roc_curves}
mapply(plot, x=initial_model_results$roc_list, main=names(initial_model_results$roc_list), print.auc=TRUE) %>% invisible
```

#### Confusion matrix

```{r initial_model_confusion}

initial_model_results$confusion

```

### Performance summary

```{r initial_model_performance}
initial_model_results$performance

```

### Histograms of class scores

```{r class_score_histograms}

plot_class_histograms(initial_model_results$test_predictions)

```

### Plot of test cases on entropy surface

Here we'll stick to a 2D representation, where the yellow background indicates regions of lower entropy.

```{r plot_initial_class_separation}

plot_class_separation(initial_model_results$test_predictions)

```

## Iterate modelling, case selection, and (pseudo) labelling

These are the cases selected by the initial model for labelling:

```{r initial_model_results_selected}

initial_model_results$selected <- select_cases(initial_model_results$model, unlabelled_knot_data_df)

initial_model_results$selected

```

```{r iterate}

new_sample <- initial_model_results$selected %>% pseudolabel_function %>% get_new_pseudolabelled_sample

current_training_set <- rbind(initial_training_set, new_sample[names(initial_training_set)])

ALREADY_EVALUATED <- initial_model_results$selected$path

iteration_results <- lapply(1:NUM_ITERATIONS, function(i){
  results <- fit_and_evaluate_model(current_training_set)
  
  candidate_cases <- unlabelled_knot_data_df[(unlabelled_knot_data_df$path %in% setdiff(unlabelled_knot_data_df$path, ALREADY_EVALUATED)),]
  results$selected <- select_cases(results$model, candidate_cases)

  ALREADY_EVALUATED <<- c(ALREADY_EVALUATED, results$selected$path)
  results$selected_labelled <- results$selected %>% pseudolabel_function
  next_sample <- results$selected %>% pseudolabel_function %>% get_new_pseudolabelled_sample
  
  current_training_set <<- rbind(current_training_set, next_sample[names(current_training_set)])

  results
})
```


```{r mean_entropy_of_selected_cases_by_iteration}
mean_entropy <- sapply(iteration_results, function(ires) mean(ires$selected$entropy))
plot(mean_entropy, type='l', main="mean entropy of selected cases by iteration")
```

These are the cases selected at each iteration, together with the scores produced by the model for that iteration. The `knot_class` column was added by the pseudolabelling function.

```{r iteration_results_selected}
lapply(iteration_results, function(ires) ires$selected_labelled)
```

This shows the change in the metrics, with each row showing an iteration. The 'negentropy' metric is the negative entropy across all three class probabilities.

```{r visualize_metrics_by_iteration}
do.call("rbind", lapply(iteration_results, function(ires) ires$performance))

```

### Visualizing improvement for actively learned model

Here we plot a series of ROC curves showing how performance changes with iterations of active learning.

```{r visualizing_improvement}

plot_roc_history("sound", initial_model_results, iteration_results)
plot_roc_history("dry", initial_model_results, iteration_results)
plot_roc_history("encased", initial_model_results, iteration_results)
```


### Final model results

```{r final_model}
final_model_results <- iteration_results[[NUM_ITERATIONS]]
```

### Confusion Matrix

```{r final_model_confusion_matrix}
final_model_results$confusion
```

### Performance summary

Summary of performance using cases selected with active learning:

```{r summary_of_preformance_using_selected_cases}

(selected_sample_results <- final_model_results$performance)
```

### Histograms of class scores for final model

```{r final_class_score_histograms}

plot_class_histograms(final_model_results$test_predictions)

```

### Entropy surface plot

```{r classifier_evolution}
plot_class_separation(final_model_results$test_predictions, main="Final")
```


## Monte Carlo Estimation of P-values

What is the probability that a set of randomly chosen cases would improve the performance of the model as much as the selected cases did? We'll add the same number of examples to the training set, except that now they will be randomly chosen. We'll repeat this sampling, training, and evaluation process `r MONTE_CARLO_SAMPLES` times, and see how many of those times we beat the performance of the selected cases.


```{r bootstrap_probability}

(N <- iteration_results[[NUM_ITERATIONS]]$tss - nrow(initial_training_set))

available_cases <- pseudolabel_function(unlabelled_knot_data_df)

random_sample_results <- sapply(1:MONTE_CARLO_SAMPLES, function(i){
  new_sample <- available_cases[sample(1:nrow(available_cases), N, replace=FALSE),]

  training_set_new <- rbind(initial_training_set, new_sample[names(initial_training_set)])

  fit_and_evaluate_model(training_set_new)$performance
}) %>% t %>% as.data.frame

```

### P-values

This table shows the fraction of times out of `r MONTE_CARLO_SAMPLES` tries that the randomly selected cases equalled or exceeded the performance of the actively learned cases for each metric. These numbers are estimated P-values.

```{r p_values}

mapply ( 
  function(metric) sum(random_sample_results[[metric]] >= selected_sample_results[[metric]]), 
  colnames(random_sample_results)
) / MONTE_CARLO_SAMPLES


```

## Model trained with all available "unlabeled" cases

For comparison, we'll build a model as though we had gone through and labeled all `r nrow(available_cases)` of the usable new examples.

```{r full_model_results}
training_set_full <- rbind(initial_training_set, available_cases[names(initial_training_set)])

full_model_results <- fit_and_evaluate_model(training_set_full)

full_model_results$confusion

full_model_results$performance

plot_class_histograms(full_model_results$test_predictions)

plot_class_separation(full_model_results$test_predictions)

```
